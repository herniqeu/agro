{"cells":[{"cell_type":"markdown","metadata":{"id":"KryM_y9vLyTH"},"source":["# Plot Segmentation in Agriculture Using Computer Vision Techniques: A Scientific Approach\n","\n","In the realm of precision agriculture, plot segmentation plays a pivotal role in crop management and yield optimization. This Jupyter Notebook presents a comprehensive workflow for segmenting agricultural plots by harnessing the power of computer vision and image processing techniques. We demonstrate the integration of image manipulation, filtering, and augmentation to extract meaningful insights from agricultural imagery.\n"]},{"cell_type":"markdown","metadata":{"id":"VN0UHe5ML2je"},"source":["# Section 1: Setup and Data Preparation\n"]},{"cell_type":"markdown","metadata":{"id":"ooClcRWfL7Hv"},"source":["## Initial Imports"]},{"cell_type":"markdown","metadata":{"id":"22KBCXnhL8I5"},"source":["Here we import necessary libraries such as OpenCV, NumPy, and Matplotlib. These libraries provide us with the tools required for image manipulation and visualization.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21856,"status":"ok","timestamp":1716505782279,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"MC-l_5pUuulC"},"outputs":[],"source":["!pip install tensorflow -q"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21269,"status":"ok","timestamp":1716505803544,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"jQNHvUmeL_sn"},"outputs":[],"source":["import os\n","import sys\n","import math\n","import random\n","import time\n","import subprocess\n","import psutil\n","\n","import cv2                          as cv\n","import keras                        as K\n","import numpy                        as np\n","import tensorflow                   as tf\n","import matplotlib.pyplot            as plt\n","\n","from PIL                            import Image, ImageTk\n","from google.colab                   import drive\n","from scipy.ndimage                  import convolve\n","\n","from tensorflow                     import data as tf_data\n","from keras                          import optimizers, callbacks, Model\n","from keras.models                   import Sequential\n","from keras.layers                   import Conv2D, Input, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D, Conv2DTranspose, UpSampling2D, add"]},{"cell_type":"markdown","metadata":{"id":"9LMMUDfXMeHN"},"source":["## Drive Mounting"]},{"cell_type":"markdown","metadata":{"id":"VU0SrWzvMkt-"},"source":["Here we mount the Google Drive to access the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28351,"status":"ok","timestamp":1716505831892,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"SEBLB-QLMgS3","outputId":"23c5d063-d03d-4854-edf8-4caae1a6b25b"},"outputs":[],"source":["drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"UKEEjbqyMty7"},"source":["## Image Data Manager Class\n","\n","The `ImageDataManager` class is designed to simplify the handling of image datasets. It loads images from specified directories, processes them into a usable format, and stores them for further analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716505831892,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"ayx56ANAMvEp"},"outputs":[],"source":["class ImageDataManager:\n","    \"\"\"\n","    A class used to manage image data for processing and analysis.\n","\n","    Attributes\n","    ----------\n","    objects : dict\n","        A dictionary to store processed image data with mask IDs as keys.\n","    base_masks_path : str\n","        Path to the directory containing mask images.\n","    base_inputs_path : str\n","        Path to the directory containing input images.\n","\n","    Methods\n","    -------\n","    process_images()\n","        Processes all images found in the base paths and populates the objects dictionary.\n","    process_input_folder(input_folder_path)\n","        Processes all images within a given input folder path and returns a list of images.\n","    \"\"\"\n","\n","    def __init__(self, base_masks_path, base_inputs_path):\n","        \"\"\"\n","        Constructs all the necessary attributes for the ImageDataManager object.\n","\n","        Parameters\n","        ----------\n","        base_masks_path : str\n","            Path to the directory containing mask images.\n","        base_inputs_path : str\n","            Path to the directory containing input images.\n","        \"\"\"\n","        self.objects = {}\n","        self.base_masks_path = base_masks_path\n","        self.base_inputs_path = base_inputs_path\n","        self.process_images()\n","\n","    @staticmethod\n","    def load_image(image_path, flags=cv.IMREAD_COLOR):\n","        \"\"\"\n","        Loads an image from a specified path.\n","\n","        Parameters\n","        ----------\n","        image_path : str\n","            Path to the image file to be loaded.\n","        flags : int\n","            Flags for image color format to be read.\n","\n","        Returns\n","        -------\n","        ndarray\n","            The image loaded into memory.\n","        \"\"\"\n","        return cv.imread(image_path, flags)\n","\n","    @staticmethod\n","    def split_channels(image_path):\n","        \"\"\"\n","        Splits the channels of an image at the given path.\n","\n","        Parameters\n","        ----------\n","        image_path : str\n","            Path to the image file.\n","\n","        Returns\n","        -------\n","        list\n","            A list containing the channels of the image.\n","        \"\"\"\n","        image = cv.imread(image_path, cv.IMREAD_UNCHANGED)\n","        channels = cv.split(image)\n","        if len(channels) == 4:\n","            return channels[:3]  # Ignore alpha channel\n","        return channels\n","\n","    def process_images(self):\n","        \"\"\"\n","        Processes the images found in the base paths and stores them in the objects dictionary.\n","        \"\"\"\n","        for mask_filename in os.listdir(self.base_masks_path):\n","            if mask_filename.endswith('.png'):\n","                mask_id = os.path.splitext(mask_filename)[0]\n","                mask_path = os.path.join(self.base_masks_path, mask_filename)\n","                mask = self.load_image(mask_path, cv.IMREAD_GRAYSCALE)\n","                input_folder_path = os.path.join(self.base_inputs_path, mask_id)\n","                images = self.process_input_folder(input_folder_path)\n","                self.objects[int(mask_id)] = {\n","                    'mask': mask,\n","                    'input': input_folder_path,\n","                    'images': images\n","                }\n","\n","    def process_input_folder(self, input_folder_path):\n","        \"\"\"\n","        Processes all images within the given input folder path.\n","\n","        Parameters\n","        ----------\n","        input_folder_path : str\n","            Path to the input folder containing image files.\n","\n","        Returns\n","        -------\n","        list\n","            A list of processed images.\n","        \"\"\"\n","        images = []\n","        if os.path.isdir(input_folder_path):\n","            for image_filename in os.listdir(input_folder_path):\n","                image_path = os.path.join(input_folder_path, image_filename)\n","                if image_filename.endswith('.png'):\n","                    channels = self.split_channels(image_path)\n","                    images.append(np.array(channels))\n","                elif image_filename.endswith(('.tif', '.tiff')):\n","                    tif_image = self.load_image(image_path, cv.IMREAD_UNCHANGED)\n","                    images.append(np.array([tif_image]))\n","        return images"]},{"cell_type":"markdown","metadata":{"id":"mZmHOWQYM7ki"},"source":["# Section 2: Visualization Tools\n"]},{"cell_type":"markdown","metadata":{"id":"Fl8de4qfM_CH"},"source":["## Visualizer Class\n","The `Visualizer` class contains methods for displaying images and their channels. It helps us understand the data and the effects of processing steps visually."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716505831893,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"Kc72vQ2rNAZ3"},"outputs":[],"source":["class Visualizer:\n","    \"\"\"\n","    A class used to visualize image data.\n","\n","    Methods\n","    -------\n","    visualize_objects(objects)\n","        Visualizes the processed image data from the objects dictionary.\n","    calculate_subplots(image_sets)\n","        Calculates the number of subplots needed for the visualization based on the image sets.\n","    display_images(num_rows, num_columns, image_sets)\n","        Displays images in a grid layout as subplots.\n","    \"\"\"\n","\n","    @staticmethod\n","    def visualize_objects(objects):\n","        \"\"\"\n","        Visualizes the processed image data from the objects dictionary.\n","\n","        Parameters\n","        ----------\n","        objects : dict\n","            Dictionary containing processed image data to visualize.\n","        \"\"\"\n","        for mask_id, data in objects.items():\n","            mask = data['mask']\n","            image_sets = data['images']\n","            num_subplots = Visualizer.calculate_subplots(image_sets)\n","            num_columns = min(num_subplots, 4)\n","            num_rows = math.ceil(num_subplots / num_columns)\n","            plt.figure(figsize=(4 * num_columns, 3 * num_rows))\n","            plt.subplot(num_rows, num_columns, 1)\n","            plt.title(f\"Mask {mask_id}\")\n","            plt.imshow(mask, cmap='gray')\n","            plt.axis('off')\n","            Visualizer.display_images(num_rows, num_columns, image_sets)\n","            plt.tight_layout()\n","            plt.show()\n","\n","    @staticmethod\n","    def calculate_subplots(image_sets):\n","        \"\"\"\n","        Calculates the number of subplots needed based on the image sets.\n","\n","        Parameters\n","        ----------\n","        image_sets : list\n","            A list of image sets, where each image set corresponds to a group of channels.\n","\n","        Returns\n","        -------\n","        int\n","            The total number of subplots needed.\n","        \"\"\"\n","        return 1 + sum(len(images) for images in image_sets)\n","\n","    @staticmethod\n","    def display_images(num_rows, num_columns, image_sets):\n","        \"\"\"\n","        Displays images in a grid layout as subplots.\n","\n","        Parameters\n","        ----------\n","        num_rows : int\n","            Number of rows in the grid layout.\n","        num_columns : int\n","            Number of columns in the grid layout.\n","        image_sets : list\n","            A list of image sets to be displayed.\n","        \"\"\"\n","        subplot_idx = 2\n","        for image_set in image_sets:\n","            for channel in image_set:\n","                plt.subplot(num_rows, num_columns, subplot_idx)\n","                plt.imshow(channel, cmap='gray')\n","                plt.axis('off')\n","                title = f\"TIFF Image {subplot_idx-1}\" if len(image_set) == 1 else f\"PNG Ch {subplot_idx-1}\"\n","                plt.title(title)\n","                subplot_idx += 1"]},{"cell_type":"markdown","metadata":{"id":"2qRfFpPqNvLQ"},"source":["# Section 3: Image Processing Pipeline"]},{"cell_type":"markdown","metadata":{"id":"rsur5fkpQhEj"},"source":["## Preprocessing Auxiliary Classes\n","\n","Here we define a suite of image preprocessing classes that extend `BaseImageProcess`. Each class implements a specific image preprocessing technique, such as rotation, blurring, and thresholding, which are essential for the feature extraction phase of plot segmentation.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716505831893,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"sQPeWW0SQnmE"},"outputs":[],"source":["class BaseImageProcess:\n","    \"\"\"\n","    BaseImageProcess: A base class for image processing algorithms.\n","\n","    This class provides a basic framework for implementing image processing algorithms and is intended to be subclassed.\n","    Subclasses should implement the `apply` method to perform specific image processing operations on an input image.\n","    \"\"\"\n","\n","    def apply(self, img, mask=None):\n","        \"\"\"\n","        Placeholder for applying an image processing algorithm.\n","\n","        Args:\n","            img: The input image to process.\n","\n","        Returns:\n","            The processed image.\n","        \"\"\"\n","        pass\n","\n","\n","class Rotate(BaseImageProcess):\n","    def __init__(self):\n","        self.angle = random.choice(list(range(-180, 181, 10)))\n","\n","    def apply(self, img, mask=None):\n","        height, width = img.shape[:2]\n","        rotation_matrix = cv.getRotationMatrix2D((width / 2, height / 2), self.angle, 1)\n","        return cv.warpAffine(img, rotation_matrix, (width, height)), (\n","            cv.warpAffine(mask, rotation_matrix, (width, height))\n","            if mask is not None\n","            else None\n","        )\n","\n","\n","class BilateralFilter(BaseImageProcess):\n","    \"\"\"\n","    BilateralFilter: Applies bilateral filtering to an image to reduce noise while keeping edges sharp.\n","    \"\"\"\n","\n","    def __init__(self, d=9, sigmaColor=75, sigmaSpace=75):\n","        self.d = d\n","        self.sigmaColor = sigmaColor\n","        self.sigmaSpace = sigmaSpace\n","\n","    def apply(self, img, mask=None):\n","        return cv.bilateralFilter(img, self.d, self.sigmaColor, self.sigmaSpace), mask\n","\n","\n","class Translate(BaseImageProcess):\n","    \"\"\"\n","    Applies translation to an image using random horizontal and vertical shifts.\n","\n","    Attributes:\n","        dx (int): Horizontal shift, chosen randomly from a specified range.\n","        dy (int): Vertical shift, chosen randomly from a specified range.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.dx = random.choice([-10, -5, 0, 5, 10])\n","        self.dy = random.choice([-10, -5, 0, 5, 10])\n","\n","    def apply(self, img, mask=None):\n","        translation_matrix = np.float32([[1, 0, self.dx], [0, 1, self.dy]])\n","        height, width = img.shape[:2]\n","        return cv.warpAffine(img, translation_matrix, (width, height)), (\n","            cv.warpAffine(mask, translation_matrix, (width, height))\n","            if mask is not None\n","            else None\n","        )\n","\n","\n","class Flip(BaseImageProcess):\n","    \"\"\"\n","    Flips an image either horizontally, vertically, or both, based on a randomly selected flip type.\n","\n","    Attributes:\n","        flip_type (int): Type of flip to apply; -1 for both axes, 0 for vertical, 1 for horizontal.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.flip_type = random.choice([-1, 0, 1])\n","\n","    def apply(self, img, mask=None):\n","        return cv.flip(img, self.flip_type), (\n","            cv.flip(mask, self.flip_type) if mask is not None else None\n","        )\n","\n","\n","class BrightnessContrast(BaseImageProcess):\n","    \"\"\"\n","    Adjusts the brightness and contrast of an image using random values.\n","\n","    Attributes:\n","        alpha (float): Factor by which the contrast will be adjusted.\n","        beta (int): Value that will be added to the pixels for brightness adjustment.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.alpha = random.uniform(0.5, 1.5)\n","        self.beta = random.randint(-50, 50)\n","\n","    def apply(self, img, mask=None):\n","        return cv.convertScaleAbs(img, alpha=self.alpha, beta=self.beta), mask\n","\n","\n","class MedianBlur(BaseImageProcess):\n","    \"\"\"\n","    Applies median blurring to an image using a randomly chosen kernel size.\n","\n","    Attributes:\n","        kernel_size (int): The size of the kernel used, selected randomly from a set of possible odd sizes.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.kernel_size = random.choice([3, 5, 7, 9, 11])\n","\n","    def apply(self, img, mask=None):\n","        return cv.medianBlur(img, self.kernel_size), mask\n","\n","\n","class RandomGaussianBlur(BaseImageProcess):\n","    \"\"\"\n","    Applies Gaussian blur filtering to an image with a randomly chosen kernel size.\n","\n","    Attributes:\n","        kernel_size (int): Size of the Gaussian blur kernel, selected randomly.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.kernel_size = random.choice([3, 5, 7, 9, 11])\n","\n","    def apply(self, img, mask=None):\n","        return cv.GaussianBlur(img, (self.kernel_size, self.kernel_size), 0), mask\n","\n","\n","class GaussianBlur(BaseImageProcess):\n","    \"\"\"\n","    GaussianBlur: Applies Gaussian blur filtering to an image.\n","\n","    This class provides an implementation of Gaussian blur filtering, commonly used to reduce image noise and detail.\n","\n","    Attributes:\n","        kernel_size (int): Size of the kernel used for the Gaussian filter.\n","    \"\"\"\n","\n","    def __init__(self, kernel_size=5):\n","        self.kernel_size = kernel_size\n","\n","    def apply(self, img, mask=None):\n","        return cv.GaussianBlur(img, (self.kernel_size, self.kernel_size), 0), mask\n","\n","\n","class BinaryThresh(BaseImageProcess):\n","    \"\"\"\n","    BinaryThresh: Applies binary thresholding to an image.\n","\n","    Binary thresholding converts an image to binary (black and white) based on a threshold value. Pixels above the\n","    threshold are set to the maximum value, and those below are set to zero.\n","\n","    Attributes:\n","        thresh (int): Threshold value.\n","        max_val (int): Maximum value to use with the threshold.\n","    \"\"\"\n","\n","    def __init__(self, thresh=127, max_val=255):\n","        self.thresh = thresh\n","        self.max_val = max_val\n","\n","    def apply(self, img, mask=None):\n","        _img = cv.cvtColor(img, cv.COLOR_BGR2GRAY) if len(img.shape) > 2 else img\n","        _, _img = cv.threshold(_img, self.thresh, self.max_val, cv.THRESH_BINARY)\n","        return _img, mask\n","\n","\n","class AdaptiveMeanThresh(BaseImageProcess):\n","    \"\"\"\n","    AdaptiveMeanThresh: Applies adaptive mean thresholding to an image.\n","\n","    Unlike simple thresholding, adaptive thresholding changes the threshold dynamically over the image based on local\n","    image characteristics.\n","\n","    Attributes:\n","        block_size (int): Size of a pixel neighborhood used to calculate the threshold.\n","        c (int): Constant subtracted from the calculated mean or weighted mean.\n","    \"\"\"\n","\n","    def __init__(self, block_size=11, c=2):\n","        self.block_size = block_size\n","        self.c = c\n","\n","    def apply(self, img, mask=None):\n","        _img = cv.cvtColor(img, cv.COLOR_BGR2GRAY) if len(img.shape) > 2 else img\n","        return (\n","            cv.adaptiveThreshold(\n","                _img,\n","                255,\n","                cv.ADAPTIVE_THRESH_MEAN_C,\n","                cv.THRESH_BINARY,\n","                self.block_size,\n","                self.c,\n","            ),\n","            mask,\n","        )\n","\n","\n","class AdaptiveGaussThresh(BaseImageProcess):\n","    \"\"\"\n","    AdaptiveGaussThresh: Applies adaptive Gaussian thresholding to an image.\n","\n","    This method uses a weighted sum of neighbourhood values where weights are a Gaussian window, which provides\n","    a more natural thresholding, especially under varying illumination.\n","\n","    Attributes:\n","        block_size (int): Size of a pixel neighborhood used to calculate the threshold.\n","        c (int): Constant subtracted from the calculated weighted sum.\n","    \"\"\"\n","\n","    def __init__(self, block_size=11, c=2):\n","        self.block_size = block_size\n","        self.c = c\n","\n","    def apply(self, img, mask=None):\n","        _img = cv.cvtColor(img, cv.COLOR_BGR2GRAY) if len(img.shape) > 2 else img\n","        return (\n","            cv.adaptiveThreshold(\n","                _img,\n","                255,\n","                cv.ADAPTIVE_THRESH_GAUSSIAN_C,\n","                cv.THRESH_BINARY,\n","                self.block_size,\n","                self.c,\n","            ),\n","            mask,\n","        )\n","\n","\n","class OtsuThresh(BaseImageProcess):\n","    \"\"\"\n","    OtsuThresh: Applies Otsu's thresholding to automatically perform histogram shape-based image thresholding.\n","\n","    This method is useful when the image contains two prominent pixel intensities and calculates an optimal threshold\n","    separating these two classes so that their combined spread (intra-class variance) is minimal.\n","    \"\"\"\n","\n","    def apply(self, img, mask=None):\n","        _img = cv.cvtColor(img, cv.COLOR_BGR2GRAY) if len(img.shape) > 2 else img\n","        _, _img = cv.threshold(_img, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n","        return _img, mask\n","\n","\n","class MorphDilate(BaseImageProcess):\n","    \"\"\"\n","    MorphDilate: Applies morphological dilation to an image.\n","\n","    Dilation increases the white region in the image or size of the foreground object. Commonly used to accentuate\n","    features.\n","\n","    Attributes:\n","        kernel_size (int): Size of the structuring element.\n","        iterations (int): Number of times dilation is applied.\n","    \"\"\"\n","\n","    def __init__(self, kernel_size=3, iterations=2):\n","        self.kernel_size = kernel_size\n","        self.iterations = iterations\n","        self.kernel = np.ones((self.kernel_size, self.kernel_size), np.uint8)\n","\n","    def apply(self, img, mask=None):\n","        return cv.dilate(img, self.kernel, iterations=self.iterations), mask\n","\n","\n","class MorphErode(BaseImageProcess):\n","    \"\"\"\n","    MorphErode: Applies morphological erosion to an image.\n","\n","    Erosion erodes away the boundaries of the foreground object and is used to diminish the features of an image.\n","\n","    Attributes:\n","        kernel_size (int): Size of the structuring element.\n","        iterations (int): Number of times erosion is applied.\n","    \"\"\"\n","\n","    def __init__(self, kernel_size=3, iterations=2):\n","        self.kernel_size = kernel_size\n","        self.iterations = iterations\n","        self.kernel = np.ones((self.kernel_size, self.kernel_size), np.uint8)\n","\n","    def apply(self, img, mask=None):\n","        return cv.erode(img, self.kernel, iterations=self.iterations), mask\n","\n","\n","class LoG(BaseImageProcess):\n","    \"\"\"\n","    LoG: Applies Laplacian of Gaussian filtering to an image.\n","\n","    This method is used to highlight regions of rapid intensity change and is therefore often used for edge detection.\n","    First, it applies a Gaussian blur, then computes the Laplacian of the result.\n","\n","    Attributes:\n","        sigma (float): Standard deviation of the Gaussian filter.\n","        size (int): Size of the filter kernel.\n","    \"\"\"\n","\n","    def __init__(self, sigma=2.0, size=None):\n","        self.sigma = sigma\n","        self.size = (\n","            size\n","            if size is not None\n","            else int(6 * self.sigma + 1) if self.sigma >= 1 else 7\n","        )\n","        if self.size % 2 == 0:\n","            self.size += 1\n","\n","    def apply(self, img, mask=None):\n","        x, y = np.meshgrid(\n","            np.arange(-self.size // 2 + 1, self.size // 2 + 1),\n","            np.arange(-self.size // 2 + 1, self.size // 2 + 1),\n","        )\n","        kernel = (\n","            -(1 / (np.pi * self.sigma**4))\n","            * (1 - ((x**2 + y**2) / (2 * self.sigma**2)))\n","            * np.exp(-(x**2 + y**2) / (2 * self.sigma**2))\n","        )\n","        kernel = kernel / np.sum(np.abs(kernel))\n","        return cv.filter2D(img, -1, kernel), mask\n","\n","\n","class LoGConv(BaseImageProcess):\n","    \"\"\"\n","    LoGConv: Implements convolution with a Laplacian of Gaussian kernel to an image.\n","\n","    Similar to the LoG class, but tailored for applying custom convolution operations directly with a manually\n","    crafted LoG kernel.\n","\n","    Attributes:\n","        sigma (float): Standard deviation of the Gaussian filter.\n","        size (int): Size of the filter kernel.\n","    \"\"\"\n","\n","    def __init__(self, sigma=2.0, size=None):\n","        self.sigma = sigma\n","        self.size = size if size is not None else int(6 * sigma + 1)\n","        if self.size % 2 == 0:\n","            self.size += 1\n","\n","    def apply(self, img, mask=None):\n","        if len(img.shape) == 3:\n","            img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n","        x, y = np.meshgrid(\n","            np.arange(-self.size // 2 + 1, self.size // 2 + 1),\n","            np.arange(-self.size // 2 + 1, self.size // 2 + 1),\n","        )\n","        kernel = (\n","            -(1 / (np.pi * self.sigma**4))\n","            * (1 - ((x**2 + y**2) / (2 * self.sigma**2)))\n","            * np.exp(-(x**2 + y**2) / (2 * self.sigma**2))\n","        )\n","        kernel = kernel / np.sum(np.abs(kernel))\n","        if len(img.shape) == 3:\n","            img = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n","            img = convolve(img, kernel)\n","            img = np.clip(img, 0, 255).astype(np.uint8)\n","        else:\n","            img = convolve(img, kernel)\n","        return img, mask"]},{"cell_type":"markdown","metadata":{"id":"DfMfFbzjQV5D"},"source":["## Processing Pipeline Class\n","\n","The `ProcessingPipeline` class orchestrates the application of filters and augmentations to the image data. It automates the process of image enhancement and prepares the data for segmentation."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716505831893,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"KNiFXNbZQVYO"},"outputs":[],"source":["class ProcessingPipeline:\n","    \"\"\"\n","    Manages the application of image processing filters and augmentations.\n","\n","    Attributes:\n","        filters (list): A list of filter objects to apply to the images.\n","        augmentations (list): A list of augmentation objects to apply to the images.\n","        history (list): Records outcomes of applied filters and augmentations for visualization.\n","    \"\"\"\n","\n","    def __init__(self, plot_storyline=False):\n","        \"\"\"\n","        Initializes the processing pipeline with empty lists for filters, augmentations, and history.\n","        \"\"\"\n","        self.filters = []\n","        self.augmentations = []\n","        self.history = []\n","\n","    def add_filters(self, filters):\n","        \"\"\"\n","        Adds multiple filter objects to the pipeline.\n","\n","        Parameters:\n","            filters (list): List of filter objects to be added.\n","        \"\"\"\n","        self.filters.extend(filters)\n","\n","    def clear_filters(self):\n","        \"\"\"\n","        Clears all filter objects from the pipeline.\n","        \"\"\"\n","        self.filters = []\n","\n","    def add_augmentations(self, augmentations):\n","        \"\"\"\n","        Adds multiple augmentation objects to the pipeline.\n","\n","        Parameters:\n","            augmentations (list): List of augmentation objects to be added.\n","        \"\"\"\n","        self.augmentations.extend(augmentations)\n","\n","    def clear_augmentations(self):\n","        \"\"\"\n","        Clears all augmentation objects from the pipeline.\n","        \"\"\"\n","        self.augmentations = []\n","\n","    def apply_filters(self, img):\n","        \"\"\"\n","        Applies each filter in sequence to the image.\n","\n","        Parameters:\n","            img (numpy.ndarray): The original image to be processed.\n","\n","        Returns:\n","            numpy.ndarray: The image processed by all filters.\n","        \"\"\"\n","        _img = img\n","        for _filter in self.filters:\n","            _img, _ = _filter.apply(_img, None)\n","            self.history.append((_img.copy(), type(_filter).__name__, \"Filter\"))\n","        return _img\n","\n","    def apply_crop(self, img, mask, new_width=120, new_height=120):\n","        \"\"\"\n","        Randomly crops the given image and mask arrays into 'n' new images and masks with dimensions 'new_width' x 'new_height'.\n","\n","        Parameters:\n","            img (numpy.ndarray): The numpy array representing the original image.\n","            mask (numpy.ndarray): The numpy array representing the original mask.\n","            new_width (int): The width of the new images and masks.\n","            new_height (int): The height of the new images and masks.\n","\n","        Returns:\n","            tuple: A tuple containing three elements:\n","                   - A list of numpy.ndarray representing the cropped images.\n","                   - A list of numpy.ndarray representing the cropped masks.\n","                   - A list of tuples containing the coordinates of each crop.\n","        \"\"\"\n","        original_height, original_width = img.shape[:2]\n","        if new_width > original_width or new_height > original_height:\n","            raise ValueError(\n","                \"New dimensions must be smaller than the original dimensions.\"\n","            )\n","\n","        cropped_images, cropped_masks, crop_coordinates = [], [], []\n","\n","        number_of_height_crops = original_height // new_height\n","        number_of_width_crops = original_width // new_width\n","\n","        for row in range(0, number_of_height_crops):\n","            for col in range(0, number_of_width_crops):\n","                top = row * new_height\n","                left = col * new_width\n","                cropped_img = img[top : top + new_height, left : left + new_width]\n","                cropped_mask = mask[top : top + new_height, left : left + new_width]\n","                cropped_images.append(cropped_img)\n","                cropped_masks.append(cropped_mask)\n","                crop_coordinates.append(\n","                    ((left, top), (left + new_width, top + new_height))\n","                )\n","\n","                self.history.append(\n","                    (cropped_img, f\"Cropped Image at ({left}, {top})\", \"Crop\")\n","                )\n","\n","        return cropped_images, cropped_masks, crop_coordinates\n","\n","    def apply_augmentations(self, images, masks, n=3):\n","        \"\"\"\n","        Applies data augmentation to a list of images and their corresponding masks.\n","\n","        Parameters:\n","            images (list of numpy.ndarray): The list of numpy arrays representing the original images.\n","            masks (list of numpy.ndarray): The list of numpy arrays representing the masks for the images.\n","            n (int): Number of augmentations to apply to each image.\n","            filters (list): List of instantiated filter classes to apply.\n","\n","        Returns:\n","            tuple: A tuple containing two elements:\n","                   - List of numpy.ndarray representing the original and augmented images.\n","                   - List of numpy.ndarray representing the original and augmented masks.\n","        \"\"\"\n","        all_images = []\n","        all_masks = []\n","\n","        for image, mask in zip(images, masks):\n","            augmented_images = [image]\n","            augmented_masks = [mask]\n","            previous_transformations = set()\n","\n","            while len(augmented_images) - 1 < n:\n","                selected_filter = random.choice(self.augmentations)\n","                transformation_key = (\n","                    type(selected_filter).__name__,\n","                    tuple(selected_filter.__dict__.values()),\n","                )\n","\n","                if transformation_key not in previous_transformations:\n","                    augmented_image, augmented_mask = selected_filter.apply(image, mask)\n","                    augmented_images.append(augmented_image)\n","                    augmented_masks.append(augmented_mask)\n","                    previous_transformations.add(transformation_key)\n","                    self.history.append(\n","                        (\n","                            augmented_image,\n","                            f\"Augmented with {type(selected_filter).__name__}\",\n","                            \"Augmentation\",\n","                        )\n","                    )\n","                    self.history.append(\n","                        (\n","                            augmented_mask,\n","                            f\"[MASK] Augmented with {type(selected_filter).__name__}\",\n","                            \"Augmentation\",\n","                        )\n","                    )\n","\n","            all_images.extend(augmented_images)\n","            all_masks.extend(augmented_masks)\n","\n","        return all_images, all_masks\n","\n","    def apply_normalization(self, imgs, masks):\n","        \"\"\"\n","        Normalizes the pixel values of images and masks to the range [0, 1].\n","\n","        Parameters:\n","            imgs (list of numpy.ndarray): The list of images to be normalized.\n","            masks (list of numpy.ndarray): The list of masks to be normalized.\n","\n","        Returns:\n","            tuple: A tuple containing two elements:\n","                   - List of numpy.ndarray representing the normalized images.\n","                   - List of numpy.ndarray representing the normalized masks.\n","        \"\"\"\n","        _imgs = []\n","        _masks = []\n","        for index, _img in enumerate(imgs):\n","            height, width, _ = _img.shape\n","            m_height, m_width = masks[index].shape\n","            norm_img = np.zeros((height, width))\n","            norm_mask = np.zeros((m_height, m_width))\n","            norm_img = cv.normalize(_img, norm_img, 0, 255, cv.NORM_MINMAX)\n","            norm_mask = cv.normalize(masks[index], norm_mask, 0, 255, cv.NORM_MINMAX)\n","            norm_img = norm_img / 255\n","            norm_mask = norm_mask / 255\n","            _imgs.append(norm_img)\n","            _masks.append(norm_mask)\n","        return _imgs, _masks\n","\n","    def run(self, img, mask, n_augmented, crop_size=120, n_crop=20):\n","        \"\"\"\n","        Executes the entire image processing pipeline.\n","\n","        Parameters:\n","            img (numpy.ndarray): The original image to process.\n","            mask (numpy.ndarray): The associated mask for the image.\n","            n_augmented (int): The number of augmented images to generate.\n","            crop_size (int): The size for cropping the images.\n","            n_crop (int): The number of crops to produce.\n","\n","        Returns:\n","            tuple: A tuple containing three elements:\n","                   - List of numpy.ndarray representing the normalized images.\n","                   - List of numpy.ndarray representing the normalized masks.\n","                   - List of tuples containing the coordinates of cropped areas.\n","        \"\"\"\n","        highlighted_img = self.apply_filters(img)\n","\n","        cropped_imgs, cropped_masks, cropped_coordinates = self.apply_crop(\n","            highlighted_img, mask, new_width=crop_size, new_height=crop_size\n","        )\n","\n","        augmented_imgs, augmented_masks = self.apply_augmentations(\n","            cropped_imgs, cropped_masks, n_augmented\n","        )\n","\n","        normalized_imgs, normalized_masks = self.apply_normalization(\n","            augmented_imgs, augmented_masks\n","        )\n","\n","        return normalized_imgs, normalized_masks, cropped_coordinates\n","\n","    def get_history(self):\n","        return self.history\n"]},{"cell_type":"markdown","metadata":{"id":"bIJh-rUEQ5nW"},"source":["# Section 4: Auxiliary Functions"]},{"cell_type":"markdown","metadata":{"id":"iyhcbCC6RBHO"},"source":["## Helper Functions\n","\n","Auxiliary functions such as `get_dataset` are defined here. These functions support the main pipeline by providing image loading capabilities and visualizing the processing history.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716505831893,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"J4jlWTjkQ-7k"},"outputs":[],"source":["def get_dataset(batch_size,\n","                img_size,\n","                input_img_arr,\n","                target_img_arr,\n","                max_dataset_len=None):\n","    \"\"\"\n","    Returns a TensorFlow Dataset.\n","\n","    Args:\n","        batch_size (int): The batch size for the dataset.\n","        img_size (tuple): The size of the images in the dataset.\n","        input_img_arr (numpy.array): Array of input images.\n","        target_img_arr (numpy.array): Array of target images.\n","        max_dataset_len (int, optional): Maximum length of the dataset. Defaults to None.\n","\n","    Returns:\n","        tf.data.Dataset: A TensorFlow dataset batched with the specified batch size.\n","    \"\"\"\n","    if max_dataset_len:\n","        input_img_arr = input_img_arr[:max_dataset_len]\n","        target_img_arr = target_img_arr[:max_dataset_len]\n","    dataset = tf_data.Dataset.from_tensor_slices((input_img_arr, target_img_arr))\n","    return dataset.batch(batch_size)\n","\n","def visualize_dataset(dataset, num_samples, title):\n","    \"\"\"\n","    Visualize the dataset with images and their corresponding masks.\n","\n","    Parameters:\n","    - dataset: The TensorFlow dataset to visualize.\n","    - num_samples: Number of samples to visualize.\n","    - title: Title for the plot.\n","    \"\"\"\n","    plt.figure(figsize=(15, num_samples * 3))\n","    for i, (image, mask) in enumerate(dataset.take(num_samples)):\n","        plt.subplot(num_samples, 2, 2 * i + 1)\n","        plt.imshow(np.squeeze(image.numpy()), cmap='gray')\n","        plt.title(f'{title} Image {i+1}')\n","        plt.axis('off')\n","\n","        plt.subplot(num_samples, 2, 2 * i + 2)\n","        plt.imshow(np.squeeze(mask.numpy()), cmap='gray')\n","        plt.title(f'{title} Mask {i+1}')\n","        plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716507542372,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"2D2U4r9KxcXl"},"outputs":[],"source":["def display_three_images(mask, image, predicted_mask):\n","  f, axarr = plt.subplots(1,3)\n","  axarr[0].imshow(mask)\n","  axarr[1].imshow(image)\n","  axarr[2].imshow(predicted_mask)"]},{"cell_type":"markdown","metadata":{"id":"rrmJnEX6RTN6"},"source":["# Section 5: Execution of Processing Pipeline"]},{"cell_type":"markdown","metadata":{"id":"wjMhNLOmSB8H"},"source":["## Pipeline Configuration\n","\n","Set up the processing pipeline by selecting the desired filters and augmentations. This configuration will determine how the images are processed and enhanced.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":115922,"status":"ok","timestamp":1716506081180,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"eotK8LxWSEY4"},"outputs":[],"source":["pipeline = ProcessingPipeline()\n","pipeline.add_augmentations([Rotate(), Translate(), Flip(), BrightnessContrast(), RandomGaussianBlur(), MedianBlur()])\n","\n","base_masks_path = \"/content/drive/MyDrive/Images/Masks\"\n","base_inputs_path = \"/content/drive/MyDrive/Images/Input\"\n","\n","image_data_manager = ImageDataManager(base_masks_path, base_inputs_path)"]},{"cell_type":"markdown","metadata":{"id":"vJW6FSwcgxWL"},"source":["## Visualizing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1TnOqYSmf8aQRApde2YPL2eIXe7_xfD1Q"},"executionInfo":{"elapsed":83605,"status":"ok","timestamp":1716506344497,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"U6H988RmcOq8","outputId":"ae3f2ba3-3725-403d-c105-4b7709b37c3d"},"outputs":[],"source":["Visualizer.visualize_objects(image_data_manager.objects)"]},{"cell_type":"markdown","metadata":{"id":"3utdv8LfhCDt"},"source":["## Loading resources as `np.array`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716506344497,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"-4NJg-iWVD9W","outputId":"b529f8eb-b597-4940-d65d-0eb4b1db3486"},"outputs":[],"source":["import numpy as np\n","import cv2\n","\n","RAW_IMAGE_SIZE = 1200\n","CROP_SIZE = 120  # RAW_IMAGE_SIZE // 10\n","IMAGE_SIZE = (CROP_SIZE, CROP_SIZE)\n","TEST_SET_SIZE_AS_PERCENTAGE = 0.2\n","BATCH_SIZE = 1\n","\n","masks_as_np_array = []\n","images_as_np_array = []\n","\n","for key in image_data_manager.objects.keys():\n","    images = image_data_manager.objects[key]['images']\n","    mask = image_data_manager.objects[key]['mask']\n","\n","    filtered_images = []\n","\n","    for image in images:\n","        if image.shape[1] == RAW_IMAGE_SIZE:\n","            filtered_images.append(np.transpose(image, (1, 2, 0)))\n","\n","    image_data_manager.objects[key]['images'] = filtered_images\n","\n","    image = filtered_images[-1]\n","    n_crop = image.shape[0] // CROP_SIZE\n","    images, masks, coordinates = pipeline.run(image, mask, crop_size=CROP_SIZE, n_crop=n_crop, n_augmented=0)\n","\n","    masks_as_np_array.extend(masks)\n","    images_as_np_array.extend(images)\n","\n","print(f\"Loaded {len(images_as_np_array)} images\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":931,"status":"ok","timestamp":1716512646219,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"Gg96b1x7IE1a"},"outputs":[],"source":["images_as_np_array = np.array(images_as_np_array)\n","masks_as_np_array = np.array(masks_as_np_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":382,"status":"ok","timestamp":1716512742459,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"LSy3Rvc9IdGD"},"outputs":[],"source":["masks_as_np_array = np.expand_dims(masks_as_np_array, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1551,"status":"ok","timestamp":1716512778404,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"8noF3BgNIhOR"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(images_as_np_array, masks_as_np_array, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"Iv-E0C1f1B4k"},"source":["## Usefull info"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":386,"status":"ok","timestamp":1716512744747,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"bCkerc1d1BBV","outputId":"37ce7b70-d468-4c93-abee-47fa97d8dc68"},"outputs":[],"source":["masks_as_np_array[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716512716229,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"AQYNJURI1GoF","outputId":"1a267a95-58de-43d0-cd5d-0f9aaf338910"},"outputs":[],"source":["images_as_np_array[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":881,"status":"ok","timestamp":1716507545560,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"UaqsgsSGwFUF","outputId":"fa971518-2dcf-4e37-e3e8-e2fb133015a2"},"outputs":[],"source":["display_three_images(masks_as_np_array[0], images_as_np_array[0], images_as_np_array[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2665,"status":"ok","timestamp":1716512509431,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"lhH3lO0IHXFS","outputId":"b60bca28-71f7-4bff-fcd9-f6158b511b45"},"outputs":[],"source":["np.asarray(images_as_np_array).shape"]},{"cell_type":"markdown","metadata":{"id":"cJuU9W-J0xod"},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2495,"status":"ok","timestamp":1716513060587,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"1vVYeI04ww8-"},"outputs":[],"source":["pretrained_model = tf.keras.applications.ResNet50(\n","    include_top=False,\n","    input_shape=(120, 120, 3),\n","    pooling='max',\n","    weights='imagenet'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1716513060587,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"b3DfmeKA279d"},"outputs":[],"source":["layer_name_to_stop_at = 'conv5_block3_out'\n","truncated_model = Model(\n","    inputs=pretrained_model.input,\n","    outputs=pretrained_model.get_layer(layer_name_to_stop_at).output\n",")\n","for layer in truncated_model.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1014,"status":"ok","timestamp":1716513185252,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"03MFqCOR30HE"},"outputs":[],"source":["resnet_model = Sequential()\n","resnet_model.add(truncated_model) # 4x4\n","resnet_model.add(Conv2DTranspose(1024, kernel_size=3, strides=2, padding='same', activation='relu')) # 8x8\n","resnet_model.add(Conv2DTranspose(512, kernel_size=3, strides=2, padding='same', activation='relu')) # 16x16\n","resnet_model.add(Conv2DTranspose(256, kernel_size=3, strides=2, padding='same', activation='relu')) # 32x32\n","resnet_model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same', activation='relu')) # 64x64\n","resnet_model.add(Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu')) # 128x128\n","# Add final Conv2DTranspose layers to reach 120x120\n","resnet_model.add(Conv2DTranspose(32, kernel_size=3, strides=1, padding='same', activation='relu'))    # Output shape: 128x128x32\n","resnet_model.add(Conv2D(1, kernel_size=9, strides=1, padding='valid', activation='sigmoid'))  # Output shape: 120x120x1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1716513185252,"user":{"displayName":"Grupo 01","userId":"05061626878844173708"},"user_tz":180},"id":"avIbHhYY6C8E"},"outputs":[],"source":["optimizer = K.optimizers.Adam(learning_rate=0.01)\n","resnet_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"d0ICnN2160K8"},"outputs":[],"source":["# history = resnet_model.fit(train_ds, validation_data=validation_ds, epochs=10)\n","history = resnet_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"]},{"cell_type":"markdown","metadata":{"id":"23RbKpM-3cqt"},"source":["## Model infos\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"e2KfX1X0I-66"},"outputs":[],"source":["resnet_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KgBkfAv73iY0"},"outputs":[],"source":["truncated_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"niCP9jbt3fOU"},"outputs":[],"source":["pretrained_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"jGImI5LFKtyp"},"source":["## Results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yO18r3zkKvqz"},"outputs":[],"source":["fig, ax1 = plt.subplots(figsize=(12, 6))\n","\n","# Plotting training & validation loss\n","ax1.plot(history.history['loss'], color='blue', label='Training Loss')\n","ax1.plot(history.history['val_loss'], color='blue', linestyle='dashed', label='Validation Loss')\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('Loss', color='blue')\n","ax1.tick_params(axis='y', labelcolor='blue')\n","\n","# Creating a secondary y-axis to plot accuracy\n","ax2 = ax1.twinx()\n","ax2.plot(history.history['accuracy'], color='green', label='Training Accuracy')\n","ax2.plot(history.history['val_accuracy'], color='green', linestyle='dashed', label='Validation Accuracy')\n","ax2.set_ylabel('Accuracy', color='green')\n","ax2.tick_params(axis='y', labelcolor='green')\n","\n","# Adding legends\n","lines, labels = ax1.get_legend_handles_labels()\n","lines2, labels2 = ax2.get_legend_handles_labels()\n","ax1.legend(lines + lines2, labels + labels2, loc='upper right')\n","\n","plt.title('Training and Validation Loss and Accuracy')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tawLanGMKwwB"},"outputs":[],"source":["train_predictions = resnet_model.predict(X_train)\n","test_predictions = resnet_model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ck9LOY8yLRzX"},"outputs":[],"source":["display_three_images(X_test[0], test_predictions[0], y_test[0])"]},{"cell_type":"markdown","metadata":{"id":"btx0zYwpOqyB"},"source":["## Saving model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojxIXL1rOsfa"},"outputs":[],"source":["resnet_model.save('/content/drive/MyDrive/models/resnet_first_try.h5')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOL5gRLLFI8M3YecbRlgSU0","collapsed_sections":["UKEEjbqyMty7","mZmHOWQYM7ki","Fl8de4qfM_CH","2qRfFpPqNvLQ","rsur5fkpQhEj","DfMfFbzjQV5D","wjMhNLOmSB8H","vJW6FSwcgxWL"],"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
