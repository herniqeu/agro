{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KryM_y9vLyTH"
      },
      "source": [
        "# Plot Segmentation in Agriculture Using Computer Vision Techniques: A Scientific Approach\n",
        "\n",
        "In the realm of precision agriculture, plot segmentation plays a pivotal role in crop management and yield optimization. This Jupyter Notebook presents a comprehensive workflow for segmenting agricultural plots by harnessing the power of computer vision and image processing techniques. We demonstrate the integration of image manipulation, filtering, and augmentation to extract meaningful insights from agricultural imagery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx4FTel7-ooR"
      },
      "source": [
        "\n",
        "## Table of Contents\n",
        "1. [Setup and Data Preparation](#section1)\n",
        "   - [Initial Imports](#subsection1-1)\n",
        "   - [Drive Mounting](#subsection1-2)\n",
        "2. [Execution of Processing Pipeline](#section2)\n",
        "   - [Pipeline Configuration](#subsection2-1)\n",
        "   - [Visualizing Objects](#subsection2-2)\n",
        "   - [Loading Resources as `np.array`](#subsection2-3)\n",
        "3. [Model Training](#section3)\n",
        "4. [Model Evaluation and Visualization](#section4)\n",
        "5. [Fine Tuning and Hyperparameter Optimization](#section5)\n",
        "   - [Early Stopping](#subsection5-1)\n",
        "   - [Optuna Optimization](#subsection5-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN0UHe5ML2je"
      },
      "source": [
        "# Section 1: Setup and Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooClcRWfL7Hv"
      },
      "source": [
        "## Initial Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22KBCXnhL8I5"
      },
      "source": [
        "Here we import necessary libraries such as OpenCV, NumPy, and Matplotlib. These libraries provide us with the tools required for image manipulation and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC-l_5pUuulC",
        "outputId": "1e8b4b96-24ff-42a3-c09b-1c3a40516472"
      },
      "outputs": [],
      "source": [
        "!pip install optuna -q\n",
        "!pip install imagehash -q\n",
        "!pip install tensorflow -q\n",
        "!pip install early_stopping -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQNHvUmeL_sn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import psutil\n",
        "import optuna\n",
        "import imagehash\n",
        "import subprocess\n",
        "\n",
        "import torch.nn.functional  as F\n",
        "import keras                as K\n",
        "import cv2                  as cv\n",
        "import numpy                as np\n",
        "import tensorflow           as tf\n",
        "import torch.nn             as nn\n",
        "import matplotlib.pyplot    as plt\n",
        "import torch.optim          as optim\n",
        "\n",
        "from google.colab           import drive\n",
        "from torchvision            import models\n",
        "from torchsummary           import summary\n",
        "from scipy.ndimage          import convolve\n",
        "from torch.utils.data       import DataLoader\n",
        "from torchvision            import transforms\n",
        "from keras.models           import Sequential\n",
        "from PIL                    import Image, ImageTk\n",
        "from tensorflow             import data as tf_data\n",
        "from torch.utils.data       import DataLoader, Dataset\n",
        "from keras                  import optimizers, callbacks, Model\n",
        "from keras.layers           import Conv2D, Input, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, SeparableConv2D, Conv2DTranspose, UpSampling2D, add\n",
        "\n",
        "\n",
        "from Helper                 import *\n",
        "from BaseImageProcess       import *\n",
        "from NeuralNetwork          import *\n",
        "from optuna.trial           import TrialState\n",
        "from Visualizer             import Visualizer\n",
        "from ImageDataManager       import ImageDataManager\n",
        "from ProcessingPipeline     import ProcessingPipeline\n",
        "from Loader                 import get_masks_and_images_as_np_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LMMUDfXMeHN"
      },
      "source": [
        "## Drive Mounting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU0SrWzvMkt-"
      },
      "source": [
        "Here we mount the Google Drive to access the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEBLB-QLMgS3",
        "outputId": "2cd4fa5b-35e8-4fc3-f85d-6f692d0fedae"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrmJnEX6RTN6"
      },
      "source": [
        "# Section 2: Execution of Processing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjMhNLOmSB8H"
      },
      "source": [
        "## Pipeline Configuration\n",
        "\n",
        "Set up the processing pipeline by selecting the desired filters and augmentations. This configuration will determine how the images are processed and enhanced.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eotK8LxWSEY4"
      },
      "outputs": [],
      "source": [
        "pipeline = ProcessingPipeline()\n",
        "pipeline.add_augmentations([Rotate(), Translate(), Flip(), BrightnessContrast(), RandomGaussianBlur(), MedianBlur()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izH50UaKUm3A"
      },
      "outputs": [],
      "source": [
        "base_masks_path = \"/content/drive/MyDrive/Images/Masks\"\n",
        "base_inputs_path = \"/content/drive/MyDrive/Images/Input\"\n",
        "image_data_manager = ImageDataManager(base_masks_path, base_inputs_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJW6FSwcgxWL"
      },
      "source": [
        "## Visualizing Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U6H988RmcOq8",
        "outputId": "08d35c47-67a5-4c31-9c73-1c46419ab30b"
      },
      "outputs": [],
      "source": [
        "Visualizer.visualize_objects(image_data_manager.objects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3utdv8LfhCDt"
      },
      "source": [
        "## Loading resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhhmI1wT_ceI"
      },
      "source": [
        "#### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4NJg-iWVD9W"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1\n",
        "\n",
        "CROP_SIZE = 120\n",
        "\n",
        "RAW_IMAGE_SIZE = 1200\n",
        "\n",
        "IMAGE_SIZE = (CROP_SIZE, CROP_SIZE)\n",
        "\n",
        "TEST_SET_SIZE_AS_PERCENTAGE = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMBVwgfZ_fCe"
      },
      "source": [
        "#### Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQKaDjdcbAzy",
        "outputId": "29c8485e-8ef7-44a5-9dfa-0c822641ee7d"
      },
      "outputs": [],
      "source": [
        "masks_as_np_array = []\n",
        "images_as_np_array = []\n",
        "\n",
        "for key in image_data_manager.objects.keys():\n",
        "    images = image_data_manager.objects[key]['images']\n",
        "    mask = image_data_manager.objects[key]['mask']\n",
        "\n",
        "    filtered_images = []\n",
        "\n",
        "    for image in images:\n",
        "        if image.shape[1] == RAW_IMAGE_SIZE:\n",
        "            image = np.transpose(image, (1, 2, 0))\n",
        "            if image.shape[2] == 1:\n",
        "                image = np.repeat(image, 3, axis=2)\n",
        "            filtered_images.append(image)\n",
        "\n",
        "    image_data_manager.objects[key]['images'] = filtered_images\n",
        "\n",
        "    image = filtered_images[-1]\n",
        "    n_crop = image.shape[0] // CROP_SIZE\n",
        "    images, masks, coordinates = pipeline.run(image, mask, crop_size=CROP_SIZE, n_crop=n_crop, n_augmented=0)\n",
        "\n",
        "    masks_as_np_array.extend(masks)\n",
        "    images_as_np_array.extend(images)\n",
        "\n",
        "print(f\"Loaded {len(images_as_np_array)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwUUtBxJ_ToF"
      },
      "source": [
        "# Section 3: Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YxXNLD_bU4k"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gxWTTNoODP7C",
        "outputId": "ea82b6a9-33fd-4985-89e9-ca7415d7e511"
      },
      "outputs": [],
      "source": [
        "model = ResUNet(n_channels=3, n_classes=2, dropout=0.13677432839622955).to(device)\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the model in train mode\n",
        "model.train()\n",
        "\n",
        "# Define the preprocessing steps\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Split your data into training and testing sets\n",
        "val_samples = int(len(images_as_np_array) * TEST_SET_SIZE_AS_PERCENTAGE)\n",
        "\n",
        "train_x = images_as_np_array[:-val_samples]\n",
        "train_y = masks_as_np_array[:-val_samples]\n",
        "\n",
        "test_x = images_as_np_array[-val_samples:]\n",
        "test_y = masks_as_np_array[-val_samples:]\n",
        "\n",
        "\n",
        "# Create PyTorch Datasets\n",
        "train_set = CustomDataset(\n",
        "    images=train_x,\n",
        "    masks=train_y,\n",
        "    transform=preprocess\n",
        ")\n",
        "\n",
        "test_set = CustomDataset(\n",
        "    images=test_x,\n",
        "    masks=test_y,\n",
        "    transform=preprocess\n",
        ")\n",
        "\n",
        "# Create PyTorch DataLoaders\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0013674142219634881)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set the model in training mode\n",
        "model.train()\n",
        "\n",
        "NUM_EPOCHS = 2000\n",
        "\n",
        "def dice_loss(pred, target, smooth = 1.):\n",
        "    pred = pred.argmax(dim=1)\n",
        "    intersection = (pred * target).sum(dim=(1,2))\n",
        "    union = pred.sum(dim=(1,2)) + target.sum(dim=(1,2))\n",
        "\n",
        "    dice = (2. * intersection + smooth) / (union + smooth)\n",
        "\n",
        "    return 1 - dice.mean()\n",
        "\n",
        "\n",
        "losses = []\n",
        "dice_coeffs = []\n",
        "\n",
        "best_coverage = 0.0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    epoch_dice = 0\n",
        "    coverage = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs = inputs.float().to(device)  # Convert inputs to Float format and move to GPU\n",
        "        targets = targets.long().to(device)  # Ensure targets are Long format for CrossEntropyLoss and move to GPU\n",
        "\n",
        "        # Forward pass\n",
        "        # outputs = model(inputs)['out']\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        d_loss = dice_loss(torch.softmax(outputs, dim=1), targets)\n",
        "        total_loss = loss + d_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate Dice coefficient\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        dice = dice_coefficient(preds, targets)\n",
        "        epoch_dice += dice.item()\n",
        "        epoch_loss += total_loss.item()\n",
        "\n",
        "        current_coverage = calculate_coverage(preds, targets)\n",
        "        if current_coverage > best_coverage:\n",
        "            best_coverage = current_coverage\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model, '/content/drive/My Drive/models/res_u_coverage'+str(current_coverage)+'_.pth')\n",
        "\n",
        "    # Print loss and Dice coefficient every epoch\n",
        "    epoch_loss = epoch_loss / len(train_loader)\n",
        "    epoch_dice = epoch_dice / len(train_loader)\n",
        "    losses.append(epoch_loss)\n",
        "    dice_coeffs.append(epoch_dice)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss:.4f}, Dice Coefficient: {epoch_dice:.4f}, Coverage: {calculate_coverage(preds, targets)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qYrI2LJ_3VO"
      },
      "source": [
        "## Finding Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1eBSPrW8_FA",
        "outputId": "ba0eac43-9abe-4a7d-a4ef-0b9ece6f2a1b"
      },
      "outputs": [],
      "source": [
        "def find_duplicates(train_data, test_data):\n",
        "    \"\"\"Find duplicate images in the training and testing sets.\"\"\"\n",
        "\n",
        "    # Compute a hash for each image in the training set\n",
        "    train_hashes = [imagehash.phash(Image.fromarray((img * 255).astype(np.uint8))) for img in train_data]\n",
        "\n",
        "    # Compute a hash for each image in the testing set\n",
        "    test_hashes = [imagehash.phash(Image.fromarray((img * 255).astype(np.uint8))) for img in test_data]\n",
        "\n",
        "    # Find duplicates\n",
        "    duplicates = [test for test in test_hashes if test in train_hashes]\n",
        "\n",
        "    return duplicates\n",
        "\n",
        "duplicates_x = find_duplicates(train_x, test_x)\n",
        "duplicates_y = find_duplicates(train_y, test_y)\n",
        "\n",
        "print(f\"Duplicate images in train_x and test_x: {len(duplicates_x)}\")\n",
        "print(f\"Duplicate masks in train_y and test_y: {len(duplicates_y)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IKdP-WzyRwL0",
        "outputId": "449e9487-033a-4134-dbb6-30028ea15c78"
      },
      "outputs": [],
      "source": [
        "def visualize_data(images, masks, title):\n",
        "    \"\"\"Function to visualize images and masks\"\"\"\n",
        "    n = 10  # number of samples to display\n",
        "    fig, ax = plt.subplots(n, 2, figsize=(10, 20))\n",
        "\n",
        "    for i in range(n):\n",
        "        ax[i, 0].imshow(images[i])\n",
        "        ax[i, 0].set_title(f'{title} Image {i+1}')\n",
        "        ax[i, 1].imshow(masks[i], cmap='gray')\n",
        "        ax[i, 1].set_title(f'{title} Mask {i+1}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize training images and masks\n",
        "visualize_data(train_x, train_y, 'Train')\n",
        "\n",
        "# Visualize testing images and masks\n",
        "visualize_data(test_x, test_y, 'Test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S8B7qx_wqMl"
      },
      "source": [
        "# Section 4: Model Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "_2YVrbih8SlP",
        "outputId": "ea18fcfd-a5ba-4760-e4b6-d391e36f09a0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses, label='Loss')\n",
        "plt.title('Loss during training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(dice_coeffs, label='Dice Coefficient')\n",
        "plt.title('Dice Coefficient during training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice Coefficient')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tZ_aaV68f78",
        "outputId": "6eb5ed7d-7187-4e4d-f1e7-c710a7d3a6b4"
      },
      "outputs": [],
      "source": [
        "coverage = calculate_coverage(preds, targets)\n",
        "print(f\"Coverage: {coverage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqb3NvFS9dyk"
      },
      "source": [
        "# Manual Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAUYrFfYDViX",
        "outputId": "5c9a3347-1a8d-4306-c8fd-6334f25ca8c3"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/content/drive/My Drive/models/res_u_coverage97.26646307573702_.pth')\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd_-7n-GYN4v"
      },
      "outputs": [],
      "source": [
        "def image_transform_predict(model, image_path, device, transform=transforms.Compose([\n",
        "    transforms.Resize((120, 120)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])):\n",
        "    # Load the image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Apply the transformations\n",
        "    img_t = transform(img)\n",
        "\n",
        "    # Create a mini-batch\n",
        "    img_t = img_t.unsqueeze(0)\n",
        "\n",
        "    # Move tensor to the device where your model is\n",
        "    img_t = img_t.to(device)\n",
        "\n",
        "    # Use your model to predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(img_t)\n",
        "\n",
        "    # Use argmax to get the most likely class for each pixel\n",
        "    _, preds = torch.max(output, dim=1)\n",
        "\n",
        "    # Move predictions to CPU and convert to numpy array\n",
        "    preds = preds.cpu().numpy()\n",
        "\n",
        "    # Plot the prediction\n",
        "    plt.imshow(preds[0], cmap='gray')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "EyHGuUec90Wf",
        "outputId": "5ccaba60-1263-43c6-a50e-16999a731c7d"
      },
      "outputs": [],
      "source": [
        "image_transform_predict(model, \"/content/drive/MyDrive/manual/1.png\", device)\n",
        "image_transform_predict(model, \"/content/drive/MyDrive/manual/2.png\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpzJmoDlAO3J"
      },
      "source": [
        "# Section 5: Fine Tuning and Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caGtI0ZqAT1u"
      },
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yib7KJQ1OEJ"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deXTs_DzAaDg"
      },
      "source": [
        "## Optuna Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLtyAiYLAew2"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)  # Dropout rate\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)  # Learning rate\n",
        "\n",
        "    # Model, criterion, optimizer\n",
        "    model = ResUNet(n_channels=3, n_classes=2, dropout=dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)  # Adjust patience as needed\n",
        "\n",
        "    val_loss = np.inf  # Initialize val_loss\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0\n",
        "        epoch_dice = 0\n",
        "\n",
        "        model.train()\n",
        "        for i, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs = inputs.float().to(device)\n",
        "            targets = targets.long().to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            d_loss = dice_loss(torch.softmax(outputs, dim=1), targets)\n",
        "            total_loss = loss + d_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            dice = dice_coefficient(preds, targets)\n",
        "            epoch_dice += dice.item()\n",
        "            epoch_loss += total_loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss / len(train_loader)\n",
        "        epoch_dice = epoch_dice / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss_temp = 0\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs = inputs.float().to(device)\n",
        "                targets = targets.long().to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                loss = criterion(outputs, targets)\n",
        "                d_loss = dice_loss(torch.softmax(outputs, dim=1), targets)\n",
        "                total_loss = loss + d_loss\n",
        "\n",
        "                val_loss_temp += total_loss.item()\n",
        "\n",
        "        val_loss_temp = val_loss_temp / len(test_loader)\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(val_loss_temp, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        val_loss = val_loss_temp  # Update val_loss only if model was evaluated\n",
        "\n",
        "    return val_loss  # Now val_loss is never None\n",
        "\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=100, timeout=600)\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoAaop4C_i4Y"
      },
      "outputs": [],
      "source": [
        "torch.save(model, '/content/drive/My Drive/model_res_u_coverage_96_38.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VN0UHe5ML2je",
        "ooClcRWfL7Hv",
        "9LMMUDfXMeHN",
        "rrmJnEX6RTN6",
        "wjMhNLOmSB8H",
        "vJW6FSwcgxWL"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
