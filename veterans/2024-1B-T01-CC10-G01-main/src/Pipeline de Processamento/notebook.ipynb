{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5a2b3c-4c1d-4362-bd2a-1f1ffaa1d514",
   "metadata": {},
   "source": [
    "# Install and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7772ee-7fbb-4a7a-84fd-798d93ff57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2686b66-1cfa-42e6-91d3-bf144fbdb8af",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Modules such as pipeline and image_process were developed by the team to provide a dynamic and flexible tool for testing multiple combinations of augmentation techniques, filters, and processes. You will be able to find the source at the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aedeadc-1a98-48d4-a758-7ed607becd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from image_process.pipeline import PipelineViewer, ProcessingPipeline\n",
    "from image_process.processes import (\n",
    "    MorphDilate,\n",
    "    Rotate,\n",
    "    Translate,\n",
    "    Flip,\n",
    "    BrightnessContrast,\n",
    "    RandomGaussianBlur,\n",
    "    GaussianBlur,\n",
    "    MedianBlur,\n",
    "    BilateralFilter,\n",
    "    AdaptiveMeanThresh\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39394e4-3391-4b29-a075-7f6060aeafd8",
   "metadata": {},
   "source": [
    "# Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7fb000-e59a-4193-9a0c-b2fe360d0da5",
   "metadata": {},
   "source": [
    "## Define auxiliary methods\n",
    "A set of configuration lines and auxiliary methods were defined to enhance the process of running the pipeline, loading the images, and logging information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd0cb7-a0ec-4c8c-b1ea-2e31aa2ce2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_image(path, color_mode=cv.IMREAD_COLOR):\n",
    "    \"\"\"Loads an image from the given path with specified color mode.\"\"\"\n",
    "    image = cv.imread(path, color_mode)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not load image from {path}\")\n",
    "    return image\n",
    "\n",
    "def display_history(history):\n",
    "        n_images = len(history)\n",
    "        n_rows = math.ceil(n_images ** 0.5)\n",
    "        n_cols = math.ceil(n_images / n_rows)\n",
    "        plt.figure(figsize=(n_cols * 4, n_rows * 4))\n",
    "        for i, (img, label, _) in enumerate(history):\n",
    "            plt.subplot(n_rows, n_cols, i + 1)\n",
    "            plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n",
    "            plt.title(label)\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "def save_to_hdf5(normalized_imgs_arr, normalized_masks_arr, cropped_coordinates_arr, img_paths, filename):\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        for index, img_path in enumerate(img_paths):\n",
    "            normalized_imgs = normalized_imgs_arr[index]\n",
    "            normalized_masks = normalized_masks_arr[index]\n",
    "            cropped_coordinates = cropped_coordinates_arr[index]\n",
    "            for idx, (img, mask, coord) in enumerate(zip(normalized_imgs, normalized_masks, cropped_coordinates)):\n",
    "                group = f.create_group(f'image_{idx}_{index}')\n",
    "                group.attrs['image_shape'] = img.shape\n",
    "                group.attrs['mask_shape'] = mask.shape\n",
    "                group.attrs['original_path'] = img_path\n",
    "                \n",
    "                if img.ndim > 2:  # RGB Image\n",
    "                    red, green, blue = img[:, :, 0].flatten(), img[:, :, 1].flatten(), img[:, :, 2].flatten()\n",
    "                    group.create_dataset('image_red', data=red, compression='gzip', compression_opts=9)\n",
    "                    group.create_dataset('image_green', data=green, compression='gzip', compression_opts=9)\n",
    "                    group.create_dataset('image_blue', data=blue, compression='gzip', compression_opts=9)\n",
    "                else:  # Grayscale Image\n",
    "                    gray = img.flatten()\n",
    "                    group.create_dataset('image_gray', data=gray, compression='gzip', compression_opts=9)\n",
    "    \n",
    "                mask_flattened = mask.flatten()\n",
    "                group.create_dataset('mask', data=mask_flattened, compression='gzip', compression_opts=9)\n",
    "                group.create_dataset('coordinates', data=np.array(coord), dtype='int32')\n",
    "\n",
    "def load_from_hdf5(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        print(\"Datasets in the HDF5 file include:\")\n",
    "        for key in f.keys():\n",
    "            group = f[key]\n",
    "            print(f\"{key} - Original path: {group.attrs['original_path']}\")\n",
    "            print(f\"Image shape: {group.attrs['image_shape']}, Mask shape: {group.attrs['mask_shape']}\")\n",
    "            for dkey in group.keys():\n",
    "                print(f\"{dkey}: shape {group[dkey].shape}\")\n",
    "\n",
    "def compress_hdf5_to_zip(hdf5_filename, output_zip_filename):\n",
    "    \"\"\"\n",
    "    Compress an HDF5 file into a ZIP file.\n",
    "\n",
    "    Args:\n",
    "        hdf5_filename (str): Path to the HDF5 file to compress.\n",
    "        output_zip_filename (str): Path and name of the output ZIP file.\n",
    "    \"\"\"\n",
    "    # Create a ZipFile object in write mode\n",
    "    with zipfile.ZipFile(output_zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add the HDF5 file to the zip file\n",
    "        zipf.write(hdf5_filename, os.path.basename(hdf5_filename))\n",
    "        print(f\"Compressed {hdf5_filename} into {output_zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7e6c1-511e-4985-847b-3381b2a6318c",
   "metadata": {},
   "source": [
    "## Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f79ab3-f40b-44d4-addd-9cb2cc3f8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_img_path = \"./test_imgs\"\n",
    "imgs_paths = [os.path.join(base_img_path, img) for img in os.listdir(base_img_path)]\n",
    "imgs = [load_image(os.path.join(base_img_path, img)) for img in os.listdir(base_img_path)]\n",
    "masks = [load_image(os.path.join(base_img_path, img), cv.IMREAD_GRAYSCALE) for img in os.listdir(base_img_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79f5bd-1f86-4504-9b7e-c4d7f0b3278f",
   "metadata": {},
   "source": [
    "## Defining the pipeline parameters\n",
    "For the initial tests, two lists were established containing filter and augmentation modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c04e74-93c4-4f92-814d-e03a4a963060",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Defining number of augmented images to generate\n",
    "n_augmented = 5\n",
    "\n",
    "filters = [GaussianBlur(), AdaptiveMeanThresh()]\n",
    "augmentations = [\n",
    "    Rotate(),\n",
    "    Translate(),\n",
    "    Flip(),\n",
    "    BrightnessContrast(),\n",
    "    RandomGaussianBlur(),\n",
    "    MedianBlur(),\n",
    "    BilateralFilter(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb70d06-5317-4556-b810-2ebcf7f45a31",
   "metadata": {},
   "source": [
    "## Setting up the pipeline\n",
    "After defining the pipeline modules, we need to create an instance of the pipe and append the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261cf50-763b-40a9-b93f-8dea32d30dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a processing pipeline\n",
    "pipeline = ProcessingPipeline()\n",
    "\n",
    "# Add filters and augmentations to the pipeline\n",
    "pipeline.add_filters(filters)\n",
    "pipeline.add_augmentations(augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c032673-bd11-43c7-ae19-e0511e295175",
   "metadata": {},
   "source": [
    "# Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b0db7-ac8c-4d44-b9fc-2b0de3c8b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_imgs_arr = []\n",
    "normalized_masks_arr = []\n",
    "cropped_coordinates_arr = []\n",
    "\n",
    "for idx, img in enumerate(imgs):\n",
    "    mask = masks[idx]\n",
    "    normalized_imgs, normalized_masks, cropped_coordinates = pipeline.run(img, mask, n_augmented, 120, 10)\n",
    "    normalized_imgs_arr.append(normalized_imgs)\n",
    "    normalized_masks_arr.append(normalized_masks)\n",
    "    cropped_coordinates_arr.append(cropped_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a47600-2d48-4c29-a719-52d0a3a49952",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sum(len(n) for n in normalized_imgs_arr)\n",
    "print(f\"Image count: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930ac72-c252-4aaf-a149-56934fe4b0d6",
   "metadata": {},
   "source": [
    "## Results\n",
    "Displayed below are the processing results up to the 100th step.\n",
    "As we have yet to annotate all base image masks, here masks are defined as a copy of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8db336-6500-4268-a938-003a8868d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_history(pipeline.history[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ccf9e-4664-42f0-82bc-271341c77286",
   "metadata": {},
   "source": [
    "## Testing\n",
    "We need to create unit tests, to ensure that our modules and classes are reliable even after making updates.\n",
    "We added tests for most classes and methods.\n",
    "The output bellow assures that all tests are returning \"ok\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010df3e9-12db-4181-9209-91eabad56d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = unittest.TestLoader()\n",
    "start_dir = 'tests'\n",
    "suite = loader.discover(start_dir=start_dir, pattern='test_*.py')\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b3180-be58-44f9-8d7c-92428fe17af2",
   "metadata": {},
   "source": [
    "## Dataset Generation\n",
    "After processing the images, for retrieving the images, we need to store them in a h5 dataset. With it, we can later rebuild the original images and load them into memory, considering the original array of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b7c80-b248-4c87-96ab-eec9ce43ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'processed_data.h5'\n",
    "save_to_hdf5(normalized_imgs_arr, normalized_masks_arr, cropped_coordinates_arr, imgs_paths, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590462ef-a319-487c-a12a-17550ef47d44",
   "metadata": {},
   "source": [
    "## Compression\n",
    "Since the dataset is too large for Github to handle (and for the fact that we are required to upload to the repository), we need to compress the output dataset to a zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d37e0-58cb-4024-b828-90779618c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = 'processed_data.zip'\n",
    "compress_hdf5_to_zip(filename, zip_file)\n",
    "os.unlink(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
